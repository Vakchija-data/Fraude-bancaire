{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c7146b-e422-4e16-acf6-c3b2f24a9e3d",
   "metadata": {},
   "source": [
    "# Découpage de table de fait et dimensions et préparer table currency manuellement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f6f5f-bf26-4226-92ac-f6eb9e711227",
   "metadata": {},
   "source": [
    "## Importation des bibilothèqes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abf0ab0d-684b-4204-a8b0-11535c4ec9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c75ca46-15e6-44dd-88fc-444047b3289f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nom du gros fichier \n",
    "input_dataset = \"synthetic_fraud_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddfaad-6bd8-4916-bedd-5f6a4376c385",
   "metadata": {},
   "source": [
    "## Traitement des valeurs unknown directement sur le fichier source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47536114-4328-41bf-bdfa-a60f93d67f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pays → capitales\n",
    "capital_map = {\n",
    "    \"UK\": \"London\",\n",
    "    \"Brazil\": \"Brasilia\",\n",
    "    \"Japan\": \"Tokyo\",\n",
    "    \"Australia\": \"Canberra\",\n",
    "    \"Nigeria\": \"Abuja\",\n",
    "    \"Germany\": \"Berlin\",\n",
    "    \"Mexico\": \"Mexico City\",\n",
    "    \"Russia\": \"Moscow\",\n",
    "    \"France\": \"Paris\",\n",
    "    \"Canada\": \"Ottawa\",\n",
    "    \"Singapore\": \"Singapore\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0c36b0-1eb1-4514-b861-fa27acca7fcc",
   "metadata": {},
   "source": [
    "### Travail sur un petit échantiollons de 100 lignes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dcfe76-a251-4ad0-8e6e-ac252efd9dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire uniquement les 100 premières lignes pour tester\n",
    "small_df = pd.read_csv(r\"C:\\Users\\vakch\\Desktop\\Formation Data Analyst\\Projet\\synthetic_fraud_data.csv\", nrows=100)\n",
    "\n",
    "# Vérifier le début du dataset\n",
    "small_df[[\"city\", \"country\"]].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23dbb3-3ef1-469c-85f4-2a1c0d2ecf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for country, capital in capital_map.items():\n",
    "    mask = (small_df[\"city\"] == \"Unknown City\") & (small_df[\"country\"] == country)\n",
    "    small_df.loc[mask, \"city\"] = capital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2aa513-a156-4d05-b71a-07553154817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df[[\"city\", \"country\"]].head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172f8d68-c363-4e28-9beb-bfa8ceaadfc7",
   "metadata": {},
   "source": [
    "### Travail sur le gros fichier initial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6d1723-4a7b-447e-af08-e6844de86280",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"transactions_clean.csv\"\n",
    "# Taille des morceaux : 500 000 lignes \n",
    "chunksize = 500_000\n",
    "first_chunk = True\n",
    "\n",
    "for chunk in pd.read_csv(input_dataset, chunksize=chunksize):\n",
    "\n",
    "    # Remplacement automatique via le dictionnaire\n",
    "    for country, capital in capital_map.items():\n",
    "        mask = (chunk[\"city\"] == \"Unknown City\") & (chunk[\"country\"] == country)\n",
    "        chunk.loc[mask, \"city\"] = capital\n",
    "\n",
    "    # Écriture progressive\n",
    "    if first_chunk:\n",
    "        chunk.to_csv(output_file, index=False, mode=\"w\")\n",
    "        first_chunk = False\n",
    "    else:\n",
    "        chunk.to_csv(output_file, index=False, mode=\"a\", header=False)\n",
    "\n",
    "    print(f\"✅ Chunk traité : {len(chunk)} lignes\")\n",
    "    \n",
    "print(\"✅✅ Fichier final entièrement corrigé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837cd407-2856-4a7c-a8f4-1aaa0f304388",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_clean = pd.read_csv(r\"C:\\Users\\vakch\\Desktop\\Formation Data Analyst\\Projet\\transactions_clean.csv\")\n",
    "dataset_clean[[\"city\", \"country\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad73a54-a378-4a28-a3d5-4ec19120f426",
   "metadata": {},
   "source": [
    "## Découpage de fichier en table de fait et tables de dimensions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81210e0-e4c0-40f4-b4cb-8b9231ecd7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunk 1 en cours de traitement...\n",
      "✅ Chunk 2 en cours de traitement...\n",
      "✅ Chunk 3 en cours de traitement...\n",
      "✅ Chunk 4 en cours de traitement...\n",
      "✅ Chunk 5 en cours de traitement...\n",
      "✅ Chunk 6 en cours de traitement...\n",
      "✅ Chunk 7 en cours de traitement...\n",
      "✅ Chunk 8 en cours de traitement...\n",
      "✅ Chunk 9 en cours de traitement...\n",
      "✅ Chunk 10 en cours de traitement...\n",
      "✅ Chunk 11 en cours de traitement...\n",
      "✅ Chunk 12 en cours de traitement...\n",
      "✅ Chunk 13 en cours de traitement...\n",
      "Découpage terminé avec succés\n"
     ]
    }
   ],
   "source": [
    "inputfile_balanced = \"transactions_rebalanced_pays.csv\"\n",
    "first_chunk = True \n",
    "chunk_number = 1\n",
    "\n",
    "chuck_size = 500_000\n",
    "\n",
    "for chunk in pd.read_csv(inputfile_balanced,chunksize=chuck_size):\n",
    "    print(f\"✅ Chunk {chunk_number} en cours de traitement...\")\n",
    "    \n",
    "    #creation des tables de dimensions (Dm)\n",
    "    df_client = chunk[[\"customer_id\"]]\n",
    "    df_card = chunk[[\"card_number\",\"card_type\"]]\n",
    "    df_merchant = chunk[[\"merchant\",\"merchant_category\",\"merchant_type\"]]\n",
    "    df_localisation = chunk[[\"city\",\"country\",\"city_size\"]]\n",
    "    df_currency = chunk[[\"country\",\"currency\"]]\n",
    "    df_channel = chunk[[\"device\",\"channel\"]]\n",
    "    df_risk_merchant = chunk[[\"merchant_category\",\"high_risk_merchant\"]]\n",
    "    \n",
    "\n",
    "    # creation de la table de faits \n",
    "    df_transactions =  chunk[[\n",
    "        \"transaction_id\", \"customer_id\",\"card_number\",\n",
    "        \"timestamp\",\"merchant\", \"amount\", \"currency\",\n",
    "        \"city\",\"card_present\",\"device\",\"device_fingerprint\",\n",
    "        \"ip_address\", \"distance_from_home\", \"merchant_category\",\n",
    "        \"transaction_hour\", \"weekend_transaction\", \n",
    "        \"velocity_last_hour\",\"is_fraud\"\n",
    "    ]]\n",
    "\n",
    "    # 1er passage -> ecriture normale (w)\n",
    "    # ensuite ajouter (a)\n",
    "    # l'en - tête est écrit une seule fois \n",
    "\n",
    "    mode = \"w\" if first_chunk else \"a\"\n",
    "    header = first_chunk\n",
    "\n",
    "    # ajout progressive des données sans saturer la RAM \n",
    "    df_client.to_csv(\"dm_client_balanced_pays.csv\", mode=mode, header=header, index=False)\n",
    "    df_card.to_csv(\"dm_card_balanced_pays.csv\", mode=mode, header=header, index=False)\n",
    "    df_merchant.to_csv(\"dm_merchant_balanced_pays.csv\", mode=mode, header=header, index=False)\n",
    "    df_localisation.to_csv(\"dm_localisation_balanced_pays.csv\", mode=mode, header=header, index=False)\n",
    "    df_currency.to_csv(\"dm_currency_balanced_pays.csv\", mode=mode, header=header, index=False)\n",
    "    df_channel.to_csv(\"dm_channel_balanced_pays.csv\", mode=mode, header=header, index=False)\n",
    "    df_risk_merchant.to_csv(\"dm_risk_merchant_balanced_pays.csv\", mode=mode, header=header, index=False)\n",
    "    df_transactions.to_csv(\"fact_transactions_balanced_pays.csv\", mode=mode, header=header, index=False)\n",
    "\n",
    "    first_chunk = False\n",
    "\n",
    "    chunk_number += 1\n",
    "print(\"Découpage terminé avec succés\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb09cd92-c396-4fe1-bb0b-e391ba40baf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "localisations_clean =  pd.read_csv(r\"C:\\Users\\vakch\\Desktop\\Formation Data Analyst\\Projet\\dm_localisation.csv\")\n",
    "localisations_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867915f9-ef2b-47cf-8ddc-0d6d552b8635",
   "metadata": {},
   "source": [
    " ### Pour la table currrency, on nettoie ici car on veut récupérer les combinaisons de pays, currency unique pour pouvoir mettre les cofficients de reversion en euros et dollars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21adfffb-5eb9-4fda-b35c-b0d36819e72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = \"dm_currency.csv\"\n",
    "output_file = \"dm_currency_sans_doublons.csv\"\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Supprimer les doublons sur (country, currency)\n",
    "df_unique = df.drop_duplicates(subset=[\"country\",\"currency\"])\n",
    "\n",
    "# Sauvegarde\n",
    "df_unique.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"✅ Doublons supprimés\")\n",
    "print(f\"Lignes finales : {len(df_unique)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349fde1e-d081-477b-9540-0cdb3fe0f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_currency_sans_doublons = pd.read_csv(r\"C:\\Users\\vakch\\Desktop\\Formation Data Analyst\\Projet\\dm_currency_sans_doublons.csv\")\n",
    "df_currency_sans_doublons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85b2c8-c244-4bb8-9e86-34203a526c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
